{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction & Motivation**\n",
        "\n",
        "I made a neural network from scratch based only on Python and NumPy so that I can truly realize how machine learning algorithms function. Instead of putting all my confidence in preimplemented libraries, I ensured that I implemented every integral part—from the weighted sums in neurons to the activation functions, loss calculation, and backpropagation. Coding these out on my own didn't just make me solidly understand linear algebra and optimization techniques, but my debugging and problem-solving skills became better as well.\n",
        "\n",
        "By creating the network from scratch, I'm demonstrating that I am capable of taking theoretical mathematical concepts and applying them as efficient, functional code. This project is a reflection of my commitment to lifelong learning and passion for innovation. It is a representation of the challenges faced in real-world applications where developing scalable, interpretable models is of utmost importance. Hiring managers and recruiters will realize that I have the technical experience, interest, and hands-on exposure necessary to hold a Data Scientist or Machine Learning Engineer position."
      ],
      "metadata": {
        "id": "3XW_H7eNZUsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper Functions**"
      ],
      "metadata": {
        "id": "UUnYkTpMYw0z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oqUF7v8QXkGa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation functions and their derivatives\n",
        "def relu(x):\n",
        "    \"\"\"Applies the ReLU activation function.\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    \"\"\"Computes the derivative of ReLU.\"\"\"\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Applies the softmax function row-wise.\"\"\"\n",
        "    exp_shifted = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_shifted / np.sum(exp_shifted, axis=1, keepdims=True)\n",
        "\n",
        "# Loss function: cross-entropy\n",
        "def cross_entropy_loss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy loss.\n",
        "    y_pred: predicted probabilities, shape (n_samples, n_classes)\n",
        "    y_true: true labels as integers, shape (n_samples,)\n",
        "    \"\"\"\n",
        "    m = y_pred.shape[0]\n",
        "    # Using a small epsilon to avoid log(0)\n",
        "    eps = 1e-15\n",
        "    correct_logprobs = -np.log(y_pred[range(m), y_true] + eps)\n",
        "    loss = np.sum(correct_logprobs) / m\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build the Neural Network Class**"
      ],
      "metadata": {
        "id": "E5y5S1pOY1MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Using He initialization for the hidden layer.\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass:\n",
        "          - Compute first linear transformation (X.W1 + b1)\n",
        "          - Apply ReLU activation\n",
        "          - Compute second linear transformation (hidden_layer.W2 + b2)\n",
        "          - Apply softmax to get probabilities\n",
        "        \"\"\"\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = relu(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = softmax(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate):\n",
        "        \"\"\"\n",
        "        Backward pass (backpropagation):\n",
        "          - Compute gradient on output layer (using cross-entropy loss derivative)\n",
        "          - Backpropagate the gradient through the network using the chain rule\n",
        "          - Update weights and biases using gradient descent\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "\n",
        "        delta2 = output.copy()\n",
        "        delta2[range(m), y] -= 1\n",
        "        delta2 /= m\n",
        "\n",
        "        # Gradients for second layer weights and biases\n",
        "        dW2 = np.dot(self.a1.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "\n",
        "        # Backpropagate into hidden layer\n",
        "        delta1 = np.dot(delta2, self.W2.T) * relu_derivative(self.z1)\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update parameters using gradient descent\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        \"\"\"Train the network over a number of epochs.\"\"\"\n",
        "        for i in range(epochs):\n",
        "            output = self.forward(X)\n",
        "            loss = cross_entropy_loss(output, y)\n",
        "            self.backward(X, y, output, learning_rate)\n",
        "            if i % 100 == 0:\n",
        "                print(f\"Epoch {i}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ioDcBLBSXqCc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing the Network on a Simple Dataset**"
      ],
      "metadata": {
        "id": "MM0hLxxCY6zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR dataset\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# For XOR, we’ll define two classes: class 0 and class 1.\n",
        "# The XOR output is 0 for [0,0] and [1,1], and 1 for [0,1] and [1,0].\n",
        "y = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Define network parameters\n",
        "input_size = 2     # 2 inputs (features)\n",
        "hidden_size = 4    # number of neurons in the hidden layer (you can experiment here)\n",
        "output_size = 2    # 2 output classes (0 and 1)\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Initialize and train the network\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "nn.train(X, y, epochs, learning_rate)\n",
        "\n",
        "# Test the network's predictions after training\n",
        "predictions = nn.forward(X)\n",
        "print(\"\\nPredicted probabilities:\")\n",
        "print(predictions)\n",
        "print(\"\\nPredicted classes:\")\n",
        "print(np.argmax(predictions, axis=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qyi9Y9EXtW2",
        "outputId": "cf559dab-0540-4980-a2b1-d5c9bfa33cc9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.7444\n",
            "Epoch 100, Loss: 0.6931\n",
            "Epoch 200, Loss: 0.6930\n",
            "Epoch 300, Loss: 0.6929\n",
            "Epoch 400, Loss: 0.6927\n",
            "Epoch 500, Loss: 0.6922\n",
            "Epoch 600, Loss: 0.6910\n",
            "Epoch 700, Loss: 0.6877\n",
            "Epoch 800, Loss: 0.6750\n",
            "Epoch 900, Loss: 0.5717\n",
            "Epoch 1000, Loss: 0.2882\n",
            "Epoch 1100, Loss: 0.1555\n",
            "Epoch 1200, Loss: 0.0941\n",
            "Epoch 1300, Loss: 0.0637\n",
            "Epoch 1400, Loss: 0.0468\n",
            "Epoch 1500, Loss: 0.0363\n",
            "Epoch 1600, Loss: 0.0293\n",
            "Epoch 1700, Loss: 0.0245\n",
            "Epoch 1800, Loss: 0.0208\n",
            "Epoch 1900, Loss: 0.0181\n",
            "Epoch 2000, Loss: 0.0159\n",
            "Epoch 2100, Loss: 0.0142\n",
            "Epoch 2200, Loss: 0.0128\n",
            "Epoch 2300, Loss: 0.0116\n",
            "Epoch 2400, Loss: 0.0106\n",
            "Epoch 2500, Loss: 0.0098\n",
            "Epoch 2600, Loss: 0.0091\n",
            "Epoch 2700, Loss: 0.0085\n",
            "Epoch 2800, Loss: 0.0079\n",
            "Epoch 2900, Loss: 0.0074\n",
            "Epoch 3000, Loss: 0.0070\n",
            "Epoch 3100, Loss: 0.0066\n",
            "Epoch 3200, Loss: 0.0062\n",
            "Epoch 3300, Loss: 0.0059\n",
            "Epoch 3400, Loss: 0.0056\n",
            "Epoch 3500, Loss: 0.0054\n",
            "Epoch 3600, Loss: 0.0051\n",
            "Epoch 3700, Loss: 0.0049\n",
            "Epoch 3800, Loss: 0.0047\n",
            "Epoch 3900, Loss: 0.0045\n",
            "Epoch 4000, Loss: 0.0043\n",
            "Epoch 4100, Loss: 0.0042\n",
            "Epoch 4200, Loss: 0.0040\n",
            "Epoch 4300, Loss: 0.0039\n",
            "Epoch 4400, Loss: 0.0037\n",
            "Epoch 4500, Loss: 0.0036\n",
            "Epoch 4600, Loss: 0.0035\n",
            "Epoch 4700, Loss: 0.0034\n",
            "Epoch 4800, Loss: 0.0033\n",
            "Epoch 4900, Loss: 0.0032\n",
            "Epoch 5000, Loss: 0.0031\n",
            "Epoch 5100, Loss: 0.0030\n",
            "Epoch 5200, Loss: 0.0029\n",
            "Epoch 5300, Loss: 0.0029\n",
            "Epoch 5400, Loss: 0.0028\n",
            "Epoch 5500, Loss: 0.0027\n",
            "Epoch 5600, Loss: 0.0026\n",
            "Epoch 5700, Loss: 0.0026\n",
            "Epoch 5800, Loss: 0.0025\n",
            "Epoch 5900, Loss: 0.0025\n",
            "Epoch 6000, Loss: 0.0024\n",
            "Epoch 6100, Loss: 0.0023\n",
            "Epoch 6200, Loss: 0.0023\n",
            "Epoch 6300, Loss: 0.0022\n",
            "Epoch 6400, Loss: 0.0022\n",
            "Epoch 6500, Loss: 0.0022\n",
            "Epoch 6600, Loss: 0.0021\n",
            "Epoch 6700, Loss: 0.0021\n",
            "Epoch 6800, Loss: 0.0020\n",
            "Epoch 6900, Loss: 0.0020\n",
            "Epoch 7000, Loss: 0.0020\n",
            "Epoch 7100, Loss: 0.0019\n",
            "Epoch 7200, Loss: 0.0019\n",
            "Epoch 7300, Loss: 0.0018\n",
            "Epoch 7400, Loss: 0.0018\n",
            "Epoch 7500, Loss: 0.0018\n",
            "Epoch 7600, Loss: 0.0018\n",
            "Epoch 7700, Loss: 0.0017\n",
            "Epoch 7800, Loss: 0.0017\n",
            "Epoch 7900, Loss: 0.0017\n",
            "Epoch 8000, Loss: 0.0016\n",
            "Epoch 8100, Loss: 0.0016\n",
            "Epoch 8200, Loss: 0.0016\n",
            "Epoch 8300, Loss: 0.0016\n",
            "Epoch 8400, Loss: 0.0015\n",
            "Epoch 8500, Loss: 0.0015\n",
            "Epoch 8600, Loss: 0.0015\n",
            "Epoch 8700, Loss: 0.0015\n",
            "Epoch 8800, Loss: 0.0015\n",
            "Epoch 8900, Loss: 0.0014\n",
            "Epoch 9000, Loss: 0.0014\n",
            "Epoch 9100, Loss: 0.0014\n",
            "Epoch 9200, Loss: 0.0014\n",
            "Epoch 9300, Loss: 0.0014\n",
            "Epoch 9400, Loss: 0.0013\n",
            "Epoch 9500, Loss: 0.0013\n",
            "Epoch 9600, Loss: 0.0013\n",
            "Epoch 9700, Loss: 0.0013\n",
            "Epoch 9800, Loss: 0.0013\n",
            "Epoch 9900, Loss: 0.0013\n",
            "\n",
            "Predicted probabilities:\n",
            "[[9.96778923e-01 3.22107705e-03]\n",
            " [6.39125200e-04 9.99360875e-01]\n",
            " [6.39113377e-04 9.99360887e-01]\n",
            " [9.99553324e-01 4.46675574e-04]]\n",
            "\n",
            "Predicted classes:\n",
            "[0 1 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Synthetic Data to test"
      ],
      "metadata": {
        "id": "YwOd5XHHYHyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_synthetic_data():\n",
        "    \"\"\"\n",
        "    Generate a synthetic dataset with 3 classes.\n",
        "    Each class will have points clustered around a different center.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    num_samples = 100\n",
        "\n",
        "    # Class 0: centered around (0, 0)\n",
        "    X0 = np.random.randn(num_samples, 2) + np.array([0, 0])\n",
        "    y0 = np.zeros(num_samples, dtype=int)\n",
        "\n",
        "    # Class 1: centered around (5, 5)\n",
        "    X1 = np.random.randn(num_samples, 2) + np.array([5, 5])\n",
        "    y1 = np.ones(num_samples, dtype=int)\n",
        "\n",
        "    # Class 2: centered around (-5, 5)\n",
        "    X2 = np.random.randn(num_samples, 2) + np.array([-5, 5])\n",
        "    y2 = np.full(num_samples, 2, dtype=int)\n",
        "\n",
        "    # Combine the data\n",
        "    X = np.vstack((X0, X1, X2))\n",
        "    y = np.concatenate((y0, y1, y2))\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Generate the dataset\n",
        "X, y = generate_synthetic_data()\n"
      ],
      "metadata": {
        "id": "pWoJfGSTYHCP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an accuracy function for evaluation\n",
        "def accuracy(y_pred, y_true):\n",
        "    predictions = np.argmax(y_pred, axis=1)\n",
        "    return np.mean(predictions == y_true)\n",
        "\n",
        "# Set network parameters for the synthetic dataset\n",
        "input_size = 2\n",
        "hidden_size = 10\n",
        "output_size = 3\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Initialize the Neural Network\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "nn.train(X, y, epochs, learning_rate)\n",
        "\n",
        "# Evaluate the network on the training data\n",
        "predictions = nn.forward(X)\n",
        "print(\"Accuracy on synthetic dataset:\", accuracy(predictions, y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqitow9CX0sH",
        "outputId": "22d527da-91d2-48a4-db54-134636583acb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 11.9137\n",
            "Epoch 100, Loss: 0.4240\n",
            "Epoch 200, Loss: 0.2610\n",
            "Epoch 300, Loss: 0.1921\n",
            "Epoch 400, Loss: 0.1568\n",
            "Epoch 500, Loss: 0.1315\n",
            "Epoch 600, Loss: 0.1124\n",
            "Epoch 700, Loss: 0.0976\n",
            "Epoch 800, Loss: 0.0859\n",
            "Epoch 900, Loss: 0.0764\n",
            "Accuracy on synthetic dataset: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try testing on real world datasets after forking the repo:"
      ],
      "metadata": {
        "id": "_8vR6EyDYmiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "metadata": {
        "id": "NmD5lR1xX2uX"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}